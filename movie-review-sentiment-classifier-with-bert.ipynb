{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Introduction\n\nHello, I'm **Wesley**, nice to meet you!ðŸ‘‹ \n\nI was just reading the IMDb reviews of [*The Super Mario Bros. Movie*](https://www.imdb.com/title/tt6718170/), I thought why don't we make a sentiment classifier to categorize movie reviews!\n\nHere we will be doing [transfer learning](https://en.wikipedia.org/wiki/Transfer_learning) on BERT [(blog)](https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html) [(paper)](https://arxiv.org/abs/1810.04805v2) with an IMDb dataset to make a sentiment classifier for movie reviews.","metadata":{}},{"cell_type":"markdown","source":"# Setup Python Libraries (pip)","metadata":{}},{"cell_type":"code","source":"# install some Python packages with pip\n\n!pip install numpy torch datasets transformers evaluate --quiet","metadata":{"execution":{"iopub.status.busy":"2023-05-15T03:48:07.756128Z","iopub.execute_input":"2023-05-15T03:48:07.756840Z","iopub.status.idle":"2023-05-15T03:48:21.961126Z","shell.execute_reply.started":"2023-05-15T03:48:07.756746Z","shell.execute_reply":"2023-05-15T03:48:21.959926Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"# let's check the version we are using\n\n!pip freeze | grep -E '^numpy|^torch|^datasets|^transformers|^evaluate'","metadata":{"execution":{"iopub.status.busy":"2023-05-15T03:48:21.963655Z","iopub.execute_input":"2023-05-15T03:48:21.964069Z","iopub.status.idle":"2023-05-15T03:48:25.280326Z","shell.execute_reply.started":"2023-05-15T03:48:21.964026Z","shell.execute_reply":"2023-05-15T03:48:25.279179Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"datasets==2.1.0\nevaluate==0.4.0\nnumpy==1.21.6\ntorch @ file:///tmp/torch/torch-1.11.0-cp37-cp37m-linux_x86_64.whl\ntorchaudio @ file:///tmp/torch/torchaudio-0.11.0-cp37-cp37m-linux_x86_64.whl\ntorchmetrics==0.10.0\ntorchtext @ file:///tmp/torch/torchtext-0.12.0-cp37-cp37m-linux_x86_64.whl\ntorchvision @ file:///tmp/torch/torchvision-0.12.0-cp37-cp37m-linux_x86_64.whl\ntransformers==4.20.1\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Create IMDB Dataset for Fine-tuning BERT","metadata":{}},{"cell_type":"markdown","source":"## Let's load the IMDB Dataset","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset\n\n# let's load the imdb dataset from huggingface\n# source: (https://huggingface.co/datasets/imdb)\n\nraw_dataset = load_dataset('imdb')\nraw_dataset","metadata":{"execution":{"iopub.status.busy":"2023-05-15T03:48:31.012728Z","iopub.execute_input":"2023-05-15T03:48:31.013495Z","iopub.status.idle":"2023-05-15T03:49:09.105410Z","shell.execute_reply.started":"2023-05-15T03:48:31.013451Z","shell.execute_reply":"2023-05-15T03:49:09.104248Z"},"trusted":true},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/1.79k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"97ac665a768b448c888603e52b07ab77"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading metadata:   0%|          | 0.00/1.05k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4711ead1a08943cc899d1f96075108b8"}},"metadata":{}},{"name":"stdout","text":"Downloading and preparing dataset imdb/plain_text (download: 80.23 MiB, generated: 127.02 MiB, post-processed: Unknown size, total: 207.25 MiB) to /root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/84.1M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ab934cdf54da4506b9dd7d5de151ed3c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/25000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/25000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating unsupervised split:   0%|          | 0/50000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Dataset imdb downloaded and prepared to /root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1. Subsequent calls will reuse this data.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"66693aa76f5647cbbc953fb6d8499ac2"}},"metadata":{}},{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['text', 'label'],\n        num_rows: 25000\n    })\n    test: Dataset({\n        features: ['text', 'label'],\n        num_rows: 25000\n    })\n    unsupervised: Dataset({\n        features: ['text', 'label'],\n        num_rows: 50000\n    })\n})"},"metadata":{}}]},{"cell_type":"markdown","source":"## Let's create the train, validation, test sets","metadata":{}},{"cell_type":"code","source":"# get train and validation set\n\ndataset = raw_dataset['train'].train_test_split(test_size=0.2, seed=42, shuffle=True)\ndataset","metadata":{"execution":{"iopub.status.busy":"2023-05-15T03:49:09.107534Z","iopub.execute_input":"2023-05-15T03:49:09.109717Z","iopub.status.idle":"2023-05-15T03:49:09.150563Z","shell.execute_reply.started":"2023-05-15T03:49:09.109684Z","shell.execute_reply":"2023-05-15T03:49:09.149444Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['text', 'label'],\n        num_rows: 20000\n    })\n    test: Dataset({\n        features: ['text', 'label'],\n        num_rows: 5000\n    })\n})"},"metadata":{}}]},{"cell_type":"code","source":"# rename validation key to 'val'\n\ndataset['val'] = dataset['test'] \ndataset","metadata":{"execution":{"iopub.status.busy":"2023-05-15T03:49:09.153052Z","iopub.execute_input":"2023-05-15T03:49:09.153602Z","iopub.status.idle":"2023-05-15T03:49:09.316534Z","shell.execute_reply.started":"2023-05-15T03:49:09.153563Z","shell.execute_reply":"2023-05-15T03:49:09.315445Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['text', 'label'],\n        num_rows: 20000\n    })\n    test: Dataset({\n        features: ['text', 'label'],\n        num_rows: 5000\n    })\n    val: Dataset({\n        features: ['text', 'label'],\n        num_rows: 5000\n    })\n})"},"metadata":{}}]},{"cell_type":"code","source":"# copy test set from raw_dataset\n\ndataset['test'] = raw_dataset['test']\ndataset","metadata":{"execution":{"iopub.status.busy":"2023-05-15T03:49:09.320212Z","iopub.execute_input":"2023-05-15T03:49:09.322483Z","iopub.status.idle":"2023-05-15T03:49:09.331268Z","shell.execute_reply.started":"2023-05-15T03:49:09.321020Z","shell.execute_reply":"2023-05-15T03:49:09.330342Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['text', 'label'],\n        num_rows: 20000\n    })\n    test: Dataset({\n        features: ['text', 'label'],\n        num_rows: 25000\n    })\n    val: Dataset({\n        features: ['text', 'label'],\n        num_rows: 5000\n    })\n})"},"metadata":{}}]},{"cell_type":"markdown","source":"## We start by tokenizing our dataset with the BERT's Fast Tokenizer","metadata":{}},{"cell_type":"code","source":"# let's import the pretrained faster tokenizer from huggingface\n# source: (https://huggingface.co/distilbert-base-uncased)\n\nfrom transformers import AutoTokenizer\n\ncheckpoint = 'distilbert-base-uncased'\ntokenizer = AutoTokenizer.from_pretrained(checkpoint, use_fast=True)\ntokenizer","metadata":{"execution":{"iopub.status.busy":"2023-05-15T03:49:09.332520Z","iopub.execute_input":"2023-05-15T03:49:09.332828Z","iopub.status.idle":"2023-05-15T03:49:11.890647Z","shell.execute_reply.started":"2023-05-15T03:49:09.332800Z","shell.execute_reply":"2023-05-15T03:49:11.889616Z"},"trusted":true},"execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c08c13ca5d344217b2a27a15701cdc6d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/483 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"88b4292ebaef4a4fb5e4f1ce35612297"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/226k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9bdd944e0a4649f1855ec92e43f28500"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/455k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4e0d948f1873403a9035f606039e5651"}},"metadata":{}},{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"PreTrainedTokenizerFast(name_or_path='distilbert-base-uncased', vocab_size=30522, model_max_len=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})"},"metadata":{}}]},{"cell_type":"code","source":"# tokenize the text in batches with truncation and padding based on BERT requirements\n\ndef tokenization(example):\n    return tokenizer(example['text'], truncation=True, padding=True)\n\ntokenized_dataset = dataset.map(tokenization, batched=True, remove_columns=['text'])\ntokenized_dataset","metadata":{"execution":{"iopub.status.busy":"2023-05-15T03:49:11.892213Z","iopub.execute_input":"2023-05-15T03:49:11.892612Z","iopub.status.idle":"2023-05-15T03:49:54.444020Z","shell.execute_reply.started":"2023-05-15T03:49:11.892573Z","shell.execute_reply":"2023-05-15T03:49:54.442892Z"},"trusted":true},"execution_count":8,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/20 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c23067e902234b1284a9b9552f076ac5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/25 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f0306f354a794a34849786a73e2ae95c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/5 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d4a1594035404372837a4df17b06925d"}},"metadata":{}},{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['label', 'input_ids', 'attention_mask'],\n        num_rows: 20000\n    })\n    test: Dataset({\n        features: ['label', 'input_ids', 'attention_mask'],\n        num_rows: 25000\n    })\n    val: Dataset({\n        features: ['label', 'input_ids', 'attention_mask'],\n        num_rows: 5000\n    })\n})"},"metadata":{}}]},{"cell_type":"markdown","source":"# Setup Training Metrics (Accuracy, F1)","metadata":{}},{"cell_type":"code","source":"import evaluate\nimport numpy as np\n\n# we setup the training to evaluate the accuracy and f1 scores\n\naccuracy_metric = evaluate.load('accuracy')\nf1_metric = evaluate.load('f1')\n\ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    predictions = np.argmax(logits, axis=-1)\n    accuracy = accuracy_metric.compute(predictions=predictions, references=labels)\n    f1 = f1_metric.compute(predictions=predictions, references=labels)\n    return {**accuracy, **f1}","metadata":{"execution":{"iopub.status.busy":"2023-05-15T03:50:36.315644Z","iopub.execute_input":"2023-05-15T03:50:36.316053Z","iopub.status.idle":"2023-05-15T03:50:46.541721Z","shell.execute_reply.started":"2023-05-15T03:50:36.316017Z","shell.execute_reply":"2023-05-15T03:50:46.540744Z"},"trusted":true},"execution_count":10,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/4.20k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cac4985a52544684b1f23b2ccc2142b7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/6.77k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0de2aee753fd426b8bb143b9cefc8cb3"}},"metadata":{}}]},{"cell_type":"markdown","source":"# Setup Training Configurations","metadata":{}},{"cell_type":"code","source":"import os\nfrom transformers import AutoModelForSequenceClassification, Trainer, TrainingArguments\n\n# get bert model with a sequence classification head for sentiment analysis\n# source: (https://huggingface.co/distilbert-base-uncased)\nmodel = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n\n# setup custom training arguments\n# 1. store training checkpoints to 'results' output directory\n# 2. fine-tune for just 1 epoch\n# 3,4. use 16 as a batch size to speed things up\n# 5. evaluate validation set every 500 steps (this is the default steps)\n# 6. load the best model based on the lowest validation loss at the end of training\ntraining_args = TrainingArguments(\n    seed=42,\n    output_dir = './results',\n    num_train_epochs = 1,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=16,\n    evaluation_strategy='steps',\n    load_best_model_at_end=True,\n)\n\n# setup trainer with custom metrics (accuracy, f1)\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_dataset['train'],\n    eval_dataset=tokenized_dataset['val'],\n    compute_metrics=compute_metrics,\n)\n\n# disable wandb logging (a v4 huggingface artifact)\nos.environ['WANDB_DISABLED']= \"true\"","metadata":{"execution":{"iopub.status.busy":"2023-05-15T03:50:55.452565Z","iopub.execute_input":"2023-05-15T03:50:55.453011Z","iopub.status.idle":"2023-05-15T03:51:09.312330Z","shell.execute_reply.started":"2023-05-15T03:50:55.452962Z","shell.execute_reply":"2023-05-15T03:51:09.311182Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/256M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"021b72c9f7d7457c8656acd6b9a22555"}},"metadata":{}},{"name":"stderr","text":"Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight']\n- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight', 'classifier.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Evaluate Unfine-tuned BERT on Test Set for a Baseline Metric\n","metadata":{}},{"cell_type":"code","source":"# let's first evaluate unfine-tuned model with test set\n\ntrainer.evaluate(tokenized_dataset['test'])","metadata":{"execution":{"iopub.status.busy":"2023-05-15T03:51:09.315024Z","iopub.execute_input":"2023-05-15T03:51:09.315846Z","iopub.status.idle":"2023-05-15T03:55:10.122560Z","shell.execute_reply.started":"2023-05-15T03:51:09.315802Z","shell.execute_reply":"2023-05-15T03:55:10.121526Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stderr","text":"***** Running Evaluation *****\n  Num examples = 25000\n  Batch size = 16\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1876' max='1563' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1563/1563 09:05]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","output_type":"stream"},{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.12.21"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"W&B syncing is set to <code>`offline`<code> in this directory.  <br/>Run <code>`wandb online`<code> or set <code>WANDB_MODE=online<code> to enable cloud syncing."},"metadata":{}},{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"{'eval_loss': 0.6933382153511047,\n 'eval_accuracy': 0.49704,\n 'eval_f1': 0.4901054339010543,\n 'eval_runtime': 234.6736,\n 'eval_samples_per_second': 106.531,\n 'eval_steps_per_second': 6.66}"},"metadata":{}}]},{"cell_type":"markdown","source":"Without fine-tuning BERT, our model currently has around **49% Accuracy** (eval_accuracy) and **49% F1** (eval_f1), which is pretty bad due to the test dataset having around 50% positive and 50% negative reviews. ðŸ˜•\n\n\nLet's make it better with transfer learning! ðŸ¦¾","metadata":{}},{"cell_type":"markdown","source":"# Fine-tune BERT with IMDb Dataset","metadata":{}},{"cell_type":"code","source":"# let's fine-tune BERT with the IMDb dataset\n\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2023-05-15T03:55:52.108580Z","iopub.execute_input":"2023-05-15T03:55:52.109097Z","iopub.status.idle":"2023-05-15T04:06:32.381308Z","shell.execute_reply.started":"2023-05-15T03:55:52.109055Z","shell.execute_reply":"2023-05-15T04:06:32.380382Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\n***** Running training *****\n  Num examples = 20000\n  Num Epochs = 1\n  Instantaneous batch size per device = 16\n  Total train batch size (w. parallel, distributed & accumulation) = 16\n  Gradient Accumulation steps = 1\n  Total optimization steps = 1250\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1250' max='1250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1250/1250 10:39, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>0.320000</td>\n      <td>0.261906</td>\n      <td>0.894200</td>\n      <td>0.890135</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.240100</td>\n      <td>0.210991</td>\n      <td>0.920000</td>\n      <td>0.920096</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"***** Running Evaluation *****\n  Num examples = 5000\n  Batch size = 16\nSaving model checkpoint to ./results/checkpoint-500\nConfiguration saved in ./results/checkpoint-500/config.json\nModel weights saved in ./results/checkpoint-500/pytorch_model.bin\n***** Running Evaluation *****\n  Num examples = 5000\n  Batch size = 16\nSaving model checkpoint to ./results/checkpoint-1000\nConfiguration saved in ./results/checkpoint-1000/config.json\nModel weights saved in ./results/checkpoint-1000/pytorch_model.bin\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\nLoading best model from ./results/checkpoint-1000 (score: 0.21099142730236053).\n","output_type":"stream"},{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=1250, training_loss=0.26875121765136717, metrics={'train_runtime': 640.22, 'train_samples_per_second': 31.239, 'train_steps_per_second': 1.952, 'total_flos': 2649347973120000.0, 'train_loss': 0.26875121765136717, 'epoch': 1.0})"},"metadata":{}}]},{"cell_type":"code","source":"# let's see how well it did in the test set\n\ntrainer.evaluate(tokenized_dataset['test'])","metadata":{"execution":{"iopub.status.busy":"2023-05-15T04:06:32.385288Z","iopub.execute_input":"2023-05-15T04:06:32.387126Z","iopub.status.idle":"2023-05-15T04:10:25.323611Z","shell.execute_reply.started":"2023-05-15T04:06:32.387084Z","shell.execute_reply":"2023-05-15T04:10:25.322679Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stderr","text":"***** Running Evaluation *****\n  Num examples = 25000\n  Batch size = 16\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1563' max='1563' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1563/1563 03:52]\n    </div>\n    "},"metadata":{}},{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"{'eval_loss': 0.2036859542131424,\n 'eval_accuracy': 0.92456,\n 'eval_f1': 0.9249263593662924,\n 'eval_runtime': 232.9235,\n 'eval_samples_per_second': 107.331,\n 'eval_steps_per_second': 6.71,\n 'epoch': 1.0}"},"metadata":{}}]},{"cell_type":"markdown","source":"Woah! We got a **92% Accuracy** (eval_accuracy) and **92% F1** (eval_f1) with just 1 epoch!ðŸ¤¯","metadata":{}},{"cell_type":"markdown","source":"# Try out some examples!","metadata":{}},{"cell_type":"code","source":"from transformers import pipeline\nimport torch\n\n# get current device with pytorch\ndevice = torch.cuda.current_device()\n\n# create pipeline for sentiment classifier with custom model and tokenizer\nsentiment_classifier = pipeline(task='sentiment-analysis', model=model, tokenizer=tokenizer, device=device)","metadata":{"execution":{"iopub.status.busy":"2023-05-15T04:10:25.325165Z","iopub.execute_input":"2023-05-15T04:10:25.325918Z","iopub.status.idle":"2023-05-15T04:10:25.530225Z","shell.execute_reply.started":"2023-05-15T04:10:25.325859Z","shell.execute_reply":"2023-05-15T04:10:25.529105Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"# let's see how our model classifies a good review\n# this is from 'justinvitelli' (https://www.imdb.com/review/rw8972952)\n\nreview = \"\"\"\nFirst off this movie is for kids and fans of Nintendo and the Mario franchise.\nI still think an adult who isnt a fan could still enjoy it but this movie is so \nfull of fan service that it will have you smiling the whole time.\nThe voice acting I was skeptical but they all work and work well too.\nJack Black is the star here. I love how they kept the story simple like all of the games.\nTruly felt like a video game on screen.\nThis movie felt like a beautifully animated amusement park ride.\nThe audio in the movie was amazing too.\nThe sounds and the score with reimagined iconic music was perfect.\nSome of the songs in the movie felt unnecessary but they worked.\nI think they should've bumped the run time to 105-120 min.\n90 min felt too short as it goes by quick.\nI havent had this much wholesome fun at the movies in a long time. \nIf youre a fan you HAVE to see it.\n\"\"\"\nsentiment_classifier(review)","metadata":{"execution":{"iopub.status.busy":"2023-05-15T04:10:25.532681Z","iopub.execute_input":"2023-05-15T04:10:25.533058Z","iopub.status.idle":"2023-05-15T04:10:25.555654Z","shell.execute_reply.started":"2023-05-15T04:10:25.533025Z","shell.execute_reply":"2023-05-15T04:10:25.554571Z"},"trusted":true},"execution_count":18,"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"[{'label': 'LABEL_1', 'score': 0.9893956780433655}]"},"metadata":{}}]},{"cell_type":"markdown","source":"That is **98% POSITIVE** (LABEL_1)! \n\njustinvitelli loves the movie!","metadata":{}},{"cell_type":"code","source":"# let's see how our model classifies a bad review\n# this is from 'industriousbug16' (https://www.imdb.com/review/rw8998214)\n\nreview = \"\"\"\nFlat, visual noise.\nFundamentally incurious. Potentially injurious.\nThe mystique generated by the characters in the games is here raked over and presented\nhaphazardly by hacks.\nA hobbled attempt to explain a long and random evolution of characters who were never meant\nto be narratised fails.\nDoing it well is near impossible when you insist on EVERY LITTLE BIT OF LORE, \nfrom the last forty years being shoehorned into 90 minutes.\nMakes little sense, shamelessly leans on member berries to stimulate older viewers but offers \nnothing else.\nI feel sad for the animators who did a sterling job, but to no end as this movie has no soul.\n\"\"\"\nsentiment_classifier(review)","metadata":{"execution":{"iopub.status.busy":"2023-05-15T04:10:25.557119Z","iopub.execute_input":"2023-05-15T04:10:25.557572Z","iopub.status.idle":"2023-05-15T04:10:25.574256Z","shell.execute_reply.started":"2023-05-15T04:10:25.557534Z","shell.execute_reply":"2023-05-15T04:10:25.572769Z"},"trusted":true},"execution_count":19,"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"[{'label': 'LABEL_0', 'score': 0.9949352145195007}]"},"metadata":{}}]},{"cell_type":"markdown","source":"That is **99% NEGATIVE** (LABEL_0)!\n\nindustriousbug16 must hate the movie very badly.","metadata":{}}]}